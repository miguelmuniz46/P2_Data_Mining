{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "    \n",
    "#Práctica realizada por Javier Guillamón y Marino Pérez\n",
    "  \n",
    "class xorMLP(object):\n",
    "    #0 0 | 0\n",
    "    #0 1 | 1\n",
    "    #1 0 | 1\n",
    "    #1 1 | 0\n",
    "    def __init__(self, learning_rate=0.2):\n",
    "        self.learning_rate = learning_rate      \n",
    "        self.y = [0,1,1,0]\n",
    "        self.X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "        self.w02 = 2* np.random.random((1,1))-1\n",
    "        self.w03 = 2* np.random.random((1,1))-1\n",
    "        self.w12 = 2* np.random.random((1,1))-1\n",
    "        self.w13 = 2* np.random.random((1,1))-1\n",
    "        self.w23 = 2* np.random.random((1,1))-1\n",
    "        self.wb2 = 2* np.random.random((1,1))-1\n",
    "        self.wb3 = 2* np.random.random((1,1))-1\n",
    "        self.Bias = 1\n",
    "        \n",
    "    #Metodo que recorre la red neuronal y devuelve su resultado    \n",
    "    def feedFordward(self, X):\n",
    "       self.x2 = self.sigmoide(self.w02*X[0] + self.w12*X[1] + self.Bias*self.wb2) #Salida de la capa oculta, Pesos*Entradas+Bias, usando la funcion sigmoide\n",
    "       x3 = self.sigmoide(self.w23*self.x2 + self.w03*X[0] + self.w13*X[1] + self.Bias*self.wb3) #Salida de la capa final\n",
    "       return x3\n",
    "    \n",
    "    #Funcion de activacion sigmoide\n",
    "    def sigmoide(self,x,sigmoide=True):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #Metodo para saber si se han alcanzado los margenes de % de acierto\n",
    "    def errorAlcanzado(self, yHat,i):\n",
    "        value = 0\n",
    "        if yHat >= 0.8: value = 1\n",
    "        elif yHat <= 0.2: value = 0\n",
    "        else: return False\n",
    "        if value==self.y[i]: return True\n",
    "        return False\n",
    "    \n",
    "    #Metodo para actualizar los valores ed los pesos\n",
    "    def Backpropagation(self):\n",
    "       i=0  #Contador de iteraciones para evitar bucles infinitos\n",
    "       yAux= [0,0,0,0]  #Vector auxiliar que almacena los resultados obtenidos de y en una iteracion del bucle\n",
    "       errorAlcanzado=False  #bool que nos indica si se han alcanzado los margenes de error\n",
    "       while errorAlcanzado==False: \n",
    "          for j in range(len(self.X)):\n",
    "            i+=1\n",
    "            x0=self.X[j][0]\n",
    "            x1=self.X[j][1]\n",
    "            yHat = self.feedFordward([x0,x1])  \n",
    "            yAux[j]=yHat\n",
    "            error = yHat * (1 - yHat) * (self.y[j] - yHat) #Error de la capa de salida\n",
    "            self.w03 +=self.learning_rate*error*self.X[j][0]\n",
    "            self.w13 +=self.learning_rate*error*self.X[j][1]\n",
    "            self.w23 +=self.learning_rate*error*self.x2\n",
    "            self.wb3 +=self.learning_rate*error*self.Bias\n",
    "            error2 = error * self.w23 * self.x2 * (1-self.x2) #Error de la capa oculta\n",
    "            self.w12 +=self.learning_rate*error2*self.X[j][1]\n",
    "            self.w02 +=self.learning_rate*error2*self.X[j][0]\n",
    "            self.wb2 +=self.learning_rate*error2*self.Bias            \n",
    "          for k in range(len(yAux)): #bucle para comprobar que todos \n",
    "            resultado = self.errorAlcanzado(yAux[k],k)\n",
    "            if resultado==False: break\n",
    "          errorAlcanzado = resultado\n",
    "          if i==100000: errorAlcanzado = True #En el caso de que se hagan 100000 iteraciones se cierra el bucle\n",
    "       print(\"Iteraciones: %d\"%i)\n",
    "    \n",
    "    def fit(self):\n",
    "        self.Backpropagation()\n",
    "  \n",
    "     \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        x = [x1, x2]\n",
    "        \"\"\"\n",
    "        print(self.feedFordward(x))\n",
    "        return '1' if (self.feedFordward(x)>=0.5) else '0' #Redondeamos los datos para mostrarlos en pantalla\n",
    "      \n",
    "class DeepMLP(object):\n",
    "    def __init__(self, layers_size, learning_rate=0.):\n",
    "        \"\"\"\n",
    "        ejemplos layers_size \n",
    "            [100, 50, 10] 100 neuronas de entrada, 50 de capa oculta 1 y 10 de salida\n",
    "            [100, 50, 20, 10] 100 neuronas de entrada, 50 de capa oculta 1, 50 de capa oculta 2 y 10 de salida\n",
    "            [100, 50, 20, 50, 10] etc.\n",
    "        \"\"\"  \n",
    "        self.layers_size = layers_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32,[None,self.layers_size[0]]) #Placeholder que almacenara una matriz[X,Y] que puede tener cualquier cantidad de valores en X, en Y tiene que tener tantos valores como se indiquen en la primera capa de layers_size\n",
    "        self.y = tf.placeholder(tf.float32,[None,self.layers_size[-1]]) #Igual que el anterior, pero en Y almacena la cantidad de valores indicados en la ultima capa de layers_size\n",
    "        #Un placeholder es una variable a la que no se le asignaran datos en otro momento\n",
    "        self.w = [] #Lista que almacena todos los pesos de la red\n",
    "        self.b = [] #Lista que almacena todos los Bias de la red\n",
    "        for i in range(len(self.layers_size)-1):\n",
    "          self.w.append(tf.Variable(2*np.random.random((self.layers_size[i],self.layers_size[i+1]))-1,dtype=tf.float32)) #inicializacion de los pesos con valores aleatorios de -1 a 1, se agrupan en matrices de tamaño nºnodosEntrada * nºnodosSalida \n",
    "          self.b.append(tf.Variable(2*np.random.random((1,self.layers_size[i+1]))-1,dtype=tf.float32)) #igual que los pesos pero en este caso solo se almacenan vectores de tamaño nºnodosSalida\n",
    "        \n",
    "        #Feedfordward de la red\n",
    "        self.yHat = self.x\n",
    "        for i in range(len(self.layers_size)-1): #Se realiza la operacion tantas veces como capas haya\n",
    "          self.yHat = tf.matmul(self.yHat,self.w[i])+self.b[i] #salida de la capa = entrada*pesos+Bias\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,logits=self.yHat)) #Determina la perdida que tiene el modelo, nos dice como de ineficiente son las predicciones de la red, ademas le aplica directamente la funcion de activacion softmax\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(cross_entropy) #Minimiza el crozz_entropy usando un descenso de gradiente\n",
    "        \n",
    "        self.sess = tf.InteractiveSession() #Crea una sesion de Tensorflow  \n",
    "        self.sess.run(tf.initialize_all_variables()) #inicializa todas la varibles creadas en tensorflow \n",
    "        #en mi caso estoy usando tf.initialize_all_variables() y aque estoy con la version 0.8 de TensorFlow, en una version mas actual se puede cambiar por tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X = entradas del conjunto de datos de entrenamiento, puede ser un batch o una sola tupla\n",
    "        Y = salidas esperadas del conjunto de datos de entrenamiento, puede ser un batch o una sola tupla\n",
    "        \"\"\"\n",
    "        porcentageAlcanzado = False #bool que nos indica si se han alcanzado el % de acierto dado\n",
    "        i=0 #Contador de iteraciones para evitar bucles infinitos\n",
    "        batchX, batchY = X.next_batch(100) #Se cogen un batch con 100 elementos aleatorios dentro de X y se dividen en datos de entrada batchX y datos esperados de salida batchY\n",
    "        self.score(batchX,batchY) #Llama a la funcion score para inicializar las variables necesarias\n",
    "        while porcentageAlcanzado == False:\n",
    "          i+=1\n",
    "          if i % 100==0: #cada 100 iteraciones se hace una actualizacion de la precision del entrenamiento \n",
    "            train_accuracy = self.accuracy.eval(feed_dict={self.x:batchX,self.y:batchY}) #Se evalua actualiza el valor de accuracy \n",
    "            print('step %d, training accuracy %g'%(i,train_accuracy))\n",
    "            if train_accuracy >= 0.98: #Si la precision alcanza un valor dado paramos el entrenamiento\n",
    "              porcentageAlcanzado = True\n",
    "          self.sess.run(self.train_step,feed_dict={self.x:batchX,self.y:batchY}) #Se actualiza el valor de train_step\n",
    "          if i >= 100000: #En el caso de llegar a 100000 iteraciones se para el entrenamiento\n",
    "            porcentageAlcanzado = True\n",
    "          batchX, batchY = X.next_batch(100) #Se coge el siguiente batch para usar en la siguiente iteracion\n",
    "        print(\"Iteraciones: %d\"%i)\n",
    "          \n",
    "    def score(self, X, Y):\n",
    "        \"\"\"\n",
    "        X = entradas del conjunto de datos de testeo, puede ser un batch o una sola tupla\n",
    "        Y = salidas esperadas del conjunto de datos de testeo, puede ser un batch o una sola tupla\n",
    "        \"\"\"\n",
    "        correct_prediction = tf.equal(tf.argmax(self.yHat,1),tf.argmax(self.y,1)) #Se determina si la predcicion realizada en yHat es correcta. yHat e Y son dos arrays de 10 elementos, cada elemento dentro del array determinala probabilidad de que el numero a predecir sea el indice del array es decir, [0,0,1] el resultado seria 2. argmax coge el valor mas grande dentro del array, por lo que se comparan el valor de yHat con mas probabilidad frente al valor real de y\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # la prediccion se saca cogiendo el array de booleanos anterior, se castea a float para poder hacer la media, ek resultado nos determina cual es la probabilidad de acertar \n",
    "        self.a =self.sess.run(self.accuracy, feed_dict={self.x:X,self.y:Y}) #Se ejecuta accuracy dentro de Tensorflow con los valores de testeo X e Y\n",
    "        print('accuracy %g'%self.a)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    print(\"XOR:\")\n",
    "    xor= xorMLP(0.2)\n",
    "    xor.fit()\n",
    "    predict00 = xor.predict([0,0])\n",
    "    predict10 = xor.predict([1,0])\n",
    "    predict01 = xor.predict([0,1])\n",
    "    predict11 = xor.predict([1,1])\n",
    "    print(\"Predict 00: \"+str(predict00))\n",
    "    print(\"Predict 10: \"+str(predict10))\n",
    "    print(\"Predict 01: \"+str(predict01))\n",
    "    print(\"Predict 11: \"+str(predict11))\n",
    "    \n",
    "    print(\"\")\n",
    "    #implementar un metodo para entrenar el DeepMLP, hay que intentar llegar a un 90 y pico % de aacierto\n",
    "    #TODO MNIST TESTS\n",
    "    print(\"DeepMLP:\")\n",
    "    dpml = DeepMLP([784,10,10,10],0.2)\n",
    "    X_train = mnist.train\n",
    "    dpml.fit(X_train)\n",
    "    X_test = mnist.test.images\n",
    "    Y_test = mnist.test.labels\n",
    "    dpml.score(X_test,Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "name": "P2.JAVIER_GUILLAMON-MARINO_PEREZ",
  "notebookId": 3.159202646296577E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
